from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchValue, SearchParams
from utilities.embeddings import JinaEmbeddings, get_embedding_model
from utilities.mongodb import MongoManager
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel, RunnableMap, RunnableSequence
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
import base64
from PIL import Image
from io import BytesIO
import os
from transformers import AutoModel

# connessione a Qdrant (Docker Locale)
client = QdrantClient(url="http://qdrant:6333")

mongo_manager = MongoManager(connection_string="mongodb://host.docker.internal:27017")

# con parametri di ricerca exact=True 
# üîç Ricerca con exact=True
# full kNN search: Qdrant confronta la query con ogni singolo vettore presente nella collection, restituendo i risultati esatti basati su distanza o similarit√† 

# Viene ignorato completamente l‚Äôindice HNSW; niente trade-off tra velocit√† e accuratezza: l‚Äôaccuratezza √® massima, ma si paga in performance 
# Qdrant
# .

# Utile per:

# Benchmark: per valutare la qualit√† di una ricerca ANN confrontandola con quella esatta 
# Qdrant
# ;

# Collection piccole: la differenza di prestazioni √® minima, ma la precisione √® totale.

# ‚ö†Ô∏è Trade-off
# Metodo	Precisione	Velocit√†	Uso consigliato
# exact=True	Massima	Molto lenta	Benchmark, collezioni piccole
# exact=False + hnsw_ef	Alta (ma approssimata)	Molto pi√π veloce	Produzione, grandi dataset



# ## üß† Cosa fa realmente `hnsw_ef=128`

# 1. **Inizia la ricerca** partendo da un punto di ingresso nel livello pi√π alto del grafo.
# 2. **Scende gradualmente** ai livelli inferiori mantenendo fino a **128 candidati** potenzialmente migliori.
# 3. Ad ogni livello:

#    * **Valuta la distanza** tra la query e i nodi candidati.
#    * **Aggiorna la coda** (priority queue) mantenendo sempre i migliori 128 candidati.
# 4. Alla fine, tra questi candidati, viene scelto il numero di risultati richiesti (`limit`).

# Quindi `128` √® la **dimensione massima della coda**, non il numero limitato di vettori finali. Serve a **bilanciare qualit√† e prestazioni** ([qdrant.tech][1], [medium.com][2]).

# ---

# ## ‚öñÔ∏è Vantaggi e svantaggi

# * ‚úÖ **Corectness maggiore**: pi√π alto √® `ef`, maggiore √® la probabilit√† che tra i candidati ci siano i vicini pi√π vicini.
# * üîÑ **Costi normali**: un `ef` alto richiede pi√π CPU e tempi di risposta maggiori, ma non scala linearmente con la dimensione della collezione.
# * üó∫Ô∏è **Ricerca efficiente**: la complessit√† √® limitata, non vengono confrontati *tutti* i vettori.

# ---

# ## üîç Esempio operativo

# * **Scenario**: cerca tra 1 milione di vettori, chiedi i top 5:

#   * Con `hnsw_ef=32` ‚Üí coda fino a 32 candidati ‚Üí rapido ma meno preciso.
#   * Con `hnsw_ef=128` ‚Üí coda fino a 128 ‚Üí pi√π accurato a scapito di pi√π calcoli.
#   * Alla fine vengono restituiti solo i top `limit` (es. 5), ma la selezione avviene tra quei 128 candidati.

# ---

# ## ‚úÖ In sintesi

# * `hnsw_ef` controlla **quanta profondit√† esplorare** nel grafo.
# * Non limita i risultati, ma **ammassa i migliori candidati prima di selezionare i finali**.
# * Serve a regolare il trade-off **accuratezza vs velocit√†/risorse**.

def retriever_jina(query: str, model: AutoModel, query_filter: Filter):
    embeddings_manager = JinaEmbeddings(model)
    emb_query = embeddings_manager.embed_query(query)
    scored_points = client.query_points(
        collection_name="hitachi",
        query=emb_query,
        query_filter=query_filter,
        search_params=SearchParams(hnsw_ef=128, exact=False),
        limit=5,
        with_payload=True
    )

    contents = []
    #Serial per ricercare in Mongo
    for pt in scored_points.points:
        doc = mongo_manager.read_from_mongo(
            query={"metadata.chunk_no": pt.payload["chunk_no"], "filename": pt.payload["filename"]},
            output_format="object",
            database_name="Leonardo", collection_name="documents"
        ) # √® una lista di dizionari
        content = doc[0]["page_content"]
        images = doc[0]["metadata"].get("images", [])

        contents.append({"page_content": content, "images": images})

    full_images = []
    for content in contents:
        if content["images"]:
            full_images.extend(content["images"])


    full_content = "\n".join(content.get("page_content", "") for content in contents)

    return full_content, full_images